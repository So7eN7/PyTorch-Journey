{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf621b36-5888-4fbe-9a16-3eb22f95feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf63ba-94ec-4aa1-9eb6-2f8a9c585a59",
   "metadata": {},
   "source": [
    "# Setting up the Dataset & Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fcb2bbe-5f0a-4861-bfdd-653861fc9e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
    "DATASET_PATH = \"./data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"./saved_models/tutorial5\"\n",
    "\n",
    "# Function for setting the seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b5e7819-552a-458a-a156-2aac31d78b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data mean tensor(0.1307)\n",
      "Data std tensor(0.3081)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST(root=DATASET_PATH, train=True, download=True)\n",
    "DATA_MEANS = (train_dataset.data / 255.0).mean(axis=(0,1,2))\n",
    "DATA_STD = (train_dataset.data / 255.0).std(axis=(0,1,2))\n",
    "print(\"Data mean\", DATA_MEANS)\n",
    "print(\"Data std\", DATA_STD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b5582-b69d-4503-acb6-9af89a9215ad",
   "metadata": {},
   "source": [
    "# Transforms ---- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c51197c4-06c4-4d34-b3d5-485910c15b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                     transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "# For training, we add some augmentation. Networks are too powerful and would overfit.\n",
    "# Images are 28x28 hence the RandomResizedCrop((28,28)...)\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomResizedCrop((28,28), scale=(0.8,1.0), ratio=(0.9,1.1)),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faea98b0-0a32-4347-9d90-e3fc7c57a04e",
   "metadata": {},
   "source": [
    "# Spliting Training & Validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a897383-6f8e-480a-b4cd-99b5c7d47cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "# We need to do a little trick because the validation set should not use the augmentation.\n",
    "train_dataset = MNIST(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = MNIST(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "set_seed(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [48000, 12000])\n",
    "set_seed(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [48000, 12000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9886d28-de19-4b03-b6cd-bff2a7a5c8a5",
   "metadata": {},
   "source": [
    "# Veryfing our normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "714d7859-a466-45d1-92bb-571fdd5b74a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean tensor([0.0666])\n",
      "Batch std tensor([0.9994])\n"
     ]
    }
   ],
   "source": [
    "# The mean should be close to 0 and the standard deviation close to 1 for each channel\n",
    "imgs, labels = next(iter(train_loader))\n",
    "print(\"Batch mean\", imgs.mean(dim=[0,2,3]))\n",
    "print(\"Batch std\", imgs.std(dim=[0,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d9943-3ebf-488c-b7f3-85bde3ffb664",
   "metadata": {},
   "source": [
    "# Checking our augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71546442-a6e4-4070-b70d-3353a2017345",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ax\u001b[38;5;241m=\u001b[39mfig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m10\u001b[39m,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,xticks\u001b[38;5;241m=\u001b[39m[],yticks\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39msqueeze(imgs[i]))\n\u001b[1;32m----> 5\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[43m_\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m(),color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'item'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(30,10))\n",
    "for i in range(len(_)):\n",
    "    ax=fig.add_subplot(2,10,i+1,xticks=[],yticks=[])\n",
    "    plt.imshow(np.squeeze(imgs[i]))\n",
    "    ax.set_title(_[i].item(),color='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c729f14-0142-4f16-b6a2-08d8c11d011c",
   "metadata": {},
   "source": [
    "# Making a Neural-Network\n",
    "## For this case we will be using a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d02a7a9-0f75-42f7-87cb-3b86a0e309aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv2=nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=1)\n",
    "        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1=nn.Linear(7*7*32, 512)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "        self.out=nn.Linear(256, 10)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1, 7*7*32)\n",
    "        x=self.dropout(x)\n",
    "        x=self.dropout(F.relu(self.fc1(x)))\n",
    "        x=self.dropout(F.relu(self.fc2(x)))\n",
    "        x=self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98acd060-e2ba-45f9-9053-06f9cfaedbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819636c-1143-46e9-9ba5-e3a8b955906c",
   "metadata": {},
   "source": [
    "# Adam optimizer and CrossEntropyLoss\n",
    "## If we did compute softmax at output we can use NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565e21e7-7ebc-45fc-aa74-06aac875636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b04df635-a841-49ab-b2ed-28dda4fd4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs=117):\n",
    "    train_loss=0.0\n",
    "    val_loss=0.0\n",
    "    train_correct=0\n",
    "    val_correct=0\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for imgs, labels in train_loader:\n",
    "            model.train()\n",
    "            # Step 1: Move input data to device\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            # Step 2: Run the model on the input data\n",
    "            output = model(imgs)\n",
    "            output = output.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
    "            # Step 3: Calculate the loss\n",
    "            loss = loss_fn(output, labels)\n",
    "            # Step 4: Perform backpropagation\n",
    "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "            optimizer.zero_grad()\n",
    "            # Perform backpropagation\n",
    "            loss.backward()\n",
    "            # Step 5: Update the parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            #Tracking Loss and Accuracy\n",
    "            train_loss+=loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            train_correct+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "    # Validation loop\n",
    "        for imgs, labels in val_loader:\n",
    "            model.eval()\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            output = model(imgs)\n",
    "            output = output.squeeze(dim=1)\n",
    "            loss = loss_fn(output, labels)\n",
    "            val_loss+=loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            val_correct+=pred.eq(labels.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "        train_loss = train_loss/len(train_loader)\n",
    "        val_loss = val_loss/len(val_loader)\n",
    "\n",
    "        print(f\"\\n Epoch: {epoch+1}\\t Training Loss: {train_loss}\\t Validation Loss: {val_loss}\\n\"\n",
    "              f\"\\t Training Accuracy: {100 * train_correct/len(train_loader.dataset)} \\t Validation Accuracy: {100* val_correct/len(val_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9955dc19-e2d1-49b0-b53f-00b37ac88004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f441b31aba40ac98af6d9a42823181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch: 1\t Training Loss: 0.9591874062220256\t Validation Loss: 0.4621896211137163\n",
      "\t Training Accuracy: 67.56666666666666 \t Validation Accuracy: 85.925\n",
      "\n",
      " Epoch: 2\t Training Loss: 0.7152869838553535\t Validation Loss: 0.4413587534087661\n",
      "\t Training Accuracy: 144.09583333333333 \t Validation Accuracy: 171.84166666666667\n",
      "\n",
      " Epoch: 3\t Training Loss: 0.6697271236944404\t Validation Loss: 0.5148389752490911\n",
      "\t Training Accuracy: 222.2375 \t Validation Accuracy: 255.01666666666668\n",
      "\n",
      " Epoch: 4\t Training Loss: 0.6450927732968775\t Validation Loss: 0.39890973979676825\n",
      "\t Training Accuracy: 301.39166666666665 \t Validation Accuracy: 342.125\n",
      "\n",
      " Epoch: 5\t Training Loss: 0.6395442654764876\t Validation Loss: 0.4826977835494623\n",
      "\t Training Accuracy: 380.48125 \t Validation Accuracy: 426.825\n",
      "\n",
      " Epoch: 6\t Training Loss: 0.6255541519774533\t Validation Loss: 0.3682155907010338\n",
      "\t Training Accuracy: 460.2729166666667 \t Validation Accuracy: 515.25\n",
      "\n",
      " Epoch: 7\t Training Loss: 0.6098794006709134\t Validation Loss: 0.3499090682634387\n",
      "\t Training Accuracy: 540.46875 \t Validation Accuracy: 603.9416666666667\n",
      "\n",
      " Epoch: 8\t Training Loss: 0.6146414999864906\t Validation Loss: 0.47577628114034176\n",
      "\t Training Accuracy: 620.4145833333333 \t Validation Accuracy: 688.375\n",
      "\n",
      " Epoch: 9\t Training Loss: 0.6033337309690853\t Validation Loss: 0.35695976664815066\n",
      "\t Training Accuracy: 700.96875 \t Validation Accuracy: 777.2166666666667\n",
      "\n",
      " Epoch: 10\t Training Loss: 0.6031237459698605\t Validation Loss: 0.32530510527552803\n",
      "\t Training Accuracy: 781.6583333333333 \t Validation Accuracy: 866.75\n",
      "\n",
      " Epoch: 11\t Training Loss: 0.598350724198161\t Validation Loss: 0.3640282680488079\n",
      "\t Training Accuracy: 862.5333333333333 \t Validation Accuracy: 955.2083333333334\n",
      "\n",
      " Epoch: 12\t Training Loss: 0.6004086762020793\t Validation Loss: 0.3857768781217874\n",
      "\t Training Accuracy: 943.1104166666667 \t Validation Accuracy: 1042.9416666666666\n",
      "\n",
      " Epoch: 13\t Training Loss: 0.5940744729914532\t Validation Loss: 0.33506827390070604\n",
      "\t Training Accuracy: 1023.8291666666667 \t Validation Accuracy: 1132.45\n",
      "\n",
      " Epoch: 14\t Training Loss: 0.5853244064257087\t Validation Loss: 0.44680015445134696\n",
      "\t Training Accuracy: 1105.0541666666666 \t Validation Accuracy: 1217.9416666666666\n",
      "\n",
      " Epoch: 15\t Training Loss: 0.5916212375953477\t Validation Loss: 0.3328894450943924\n",
      "\t Training Accuracy: 1186.1833333333334 \t Validation Accuracy: 1307.3416666666667\n",
      "\n",
      " Epoch: 16\t Training Loss: 0.5941264637777526\t Validation Loss: 0.3496350569131963\n",
      "\t Training Accuracy: 1267.10625 \t Validation Accuracy: 1396.2166666666667\n",
      "\n",
      " Epoch: 17\t Training Loss: 0.5787016816956385\t Validation Loss: 0.37169032297933674\n",
      "\t Training Accuracy: 1348.5583333333334 \t Validation Accuracy: 1484.6583333333333\n",
      "\n",
      " Epoch: 18\t Training Loss: 0.5837743225277855\t Validation Loss: 0.33131492904128806\n",
      "\t Training Accuracy: 1430.0291666666667 \t Validation Accuracy: 1574.35\n",
      "\n",
      " Epoch: 19\t Training Loss: 0.5687981715050224\t Validation Loss: 0.3267541547585994\n",
      "\t Training Accuracy: 1511.6895833333333 \t Validation Accuracy: 1663.8083333333334\n",
      "\n",
      " Epoch: 20\t Training Loss: 0.5783410779291384\t Validation Loss: 0.35455833758019545\n",
      "\t Training Accuracy: 1593.2020833333333 \t Validation Accuracy: 1752.775\n",
      "\n",
      " Epoch: 21\t Training Loss: 0.5820147333440924\t Validation Loss: 0.40456355906178587\n",
      "\t Training Accuracy: 1674.5979166666666 \t Validation Accuracy: 1839.85\n",
      "\n",
      " Epoch: 22\t Training Loss: 0.5784366094106828\t Validation Loss: 0.34348361429769203\n",
      "\t Training Accuracy: 1756.0916666666667 \t Validation Accuracy: 1929.0666666666666\n",
      "\n",
      " Epoch: 23\t Training Loss: 0.5817774119279211\t Validation Loss: 0.4144745981373262\n",
      "\t Training Accuracy: 1837.6270833333333 \t Validation Accuracy: 2015.8833333333334\n",
      "\n",
      " Epoch: 24\t Training Loss: 0.5796536953932183\t Validation Loss: 0.33199733914813023\n",
      "\t Training Accuracy: 1918.9 \t Validation Accuracy: 2105.483333333333\n",
      "\n",
      " Epoch: 25\t Training Loss: 0.5691075342898068\t Validation Loss: 0.3788507874470527\n",
      "\t Training Accuracy: 2000.5375 \t Validation Accuracy: 2192.9166666666665\n",
      "\n",
      " Epoch: 26\t Training Loss: 0.5756480658896775\t Validation Loss: 0.35107472316557375\n",
      "\t Training Accuracy: 2082.102083333333 \t Validation Accuracy: 2281.85\n",
      "\n",
      " Epoch: 27\t Training Loss: 0.5693267391429605\t Validation Loss: 0.37081269061482275\n",
      "\t Training Accuracy: 2163.945833333333 \t Validation Accuracy: 2370.425\n",
      "\n",
      " Epoch: 28\t Training Loss: 0.5767670893156015\t Validation Loss: 0.33421483674327496\n",
      "\t Training Accuracy: 2245.69375 \t Validation Accuracy: 2459.8166666666666\n",
      "\n",
      " Epoch: 29\t Training Loss: 0.5752872402730209\t Validation Loss: 0.37449275559976164\n",
      "\t Training Accuracy: 2327.2229166666666 \t Validation Accuracy: 2547.2083333333335\n",
      "\n",
      " Epoch: 30\t Training Loss: 0.5806637722320722\t Validation Loss: 0.3360268612181458\n",
      "\t Training Accuracy: 2408.679166666667 \t Validation Accuracy: 2636.05\n",
      "\n",
      " Epoch: 31\t Training Loss: 0.5678610320933838\t Validation Loss: 0.32243911809899634\n",
      "\t Training Accuracy: 2490.647916666667 \t Validation Accuracy: 2725.525\n",
      "\n",
      " Epoch: 32\t Training Loss: 0.5650642265786193\t Validation Loss: 0.3587035833084814\n",
      "\t Training Accuracy: 2572.86875 \t Validation Accuracy: 2813.8166666666666\n",
      "\n",
      " Epoch: 33\t Training Loss: 0.584275602194974\t Validation Loss: 0.351749241350268\n",
      "\t Training Accuracy: 2654.55625 \t Validation Accuracy: 2902.5583333333334\n",
      "\n",
      " Epoch: 34\t Training Loss: 0.562839439746786\t Validation Loss: 0.4091200238846525\n",
      "\t Training Accuracy: 2736.7 \t Validation Accuracy: 2989.508333333333\n",
      "\n",
      " Epoch: 35\t Training Loss: 0.5892284392283679\t Validation Loss: 0.3166752746198013\n",
      "\t Training Accuracy: 2817.8645833333335 \t Validation Accuracy: 3079.4583333333335\n",
      "\n",
      " Epoch: 36\t Training Loss: 0.565765091307684\t Validation Loss: 0.3128731513810638\n",
      "\t Training Accuracy: 2899.727083333333 \t Validation Accuracy: 3169.5333333333333\n",
      "\n",
      " Epoch: 37\t Training Loss: 0.5687190175282365\t Validation Loss: 0.3053287161102452\n",
      "\t Training Accuracy: 2981.741666666667 \t Validation Accuracy: 3259.9416666666666\n",
      "\n",
      " Epoch: 38\t Training Loss: 0.5706286389510439\t Validation Loss: 0.3306804829272718\n",
      "\t Training Accuracy: 3063.50625 \t Validation Accuracy: 3349.883333333333\n",
      "\n",
      " Epoch: 39\t Training Loss: 0.5773737304896462\t Validation Loss: 0.32929726619609817\n",
      "\t Training Accuracy: 3145.2708333333335 \t Validation Accuracy: 3439.8166666666666\n",
      "\n",
      " Epoch: 40\t Training Loss: 0.5742123337251585\t Validation Loss: 0.3371894645647682\n",
      "\t Training Accuracy: 3226.891666666667 \t Validation Accuracy: 3529.0583333333334\n",
      "\n",
      " Epoch: 41\t Training Loss: 0.5825301459396316\t Validation Loss: 0.37847648464614253\n",
      "\t Training Accuracy: 3308.366666666667 \t Validation Accuracy: 3617.1\n",
      "\n",
      " Epoch: 42\t Training Loss: 0.5615374069348877\t Validation Loss: 0.33252681917575383\n",
      "\t Training Accuracy: 3390.39375 \t Validation Accuracy: 3706.7833333333333\n",
      "\n",
      " Epoch: 43\t Training Loss: 0.5633484348246617\t Validation Loss: 0.3230434786265864\n",
      "\t Training Accuracy: 3472.5291666666667 \t Validation Accuracy: 3796.45\n",
      "\n",
      " Epoch: 44\t Training Loss: 0.5743436176577116\t Validation Loss: 0.33348119478803345\n",
      "\t Training Accuracy: 3554.3 \t Validation Accuracy: 3886.233333333333\n",
      "\n",
      " Epoch: 45\t Training Loss: 0.5607537843277717\t Validation Loss: 0.30496405828309625\n",
      "\t Training Accuracy: 3636.33125 \t Validation Accuracy: 3977.2916666666665\n",
      "\n",
      " Epoch: 46\t Training Loss: 0.5556337698876833\t Validation Loss: 0.36806802856037724\n",
      "\t Training Accuracy: 3718.54375 \t Validation Accuracy: 4065.991666666667\n",
      "\n",
      " Epoch: 47\t Training Loss: 0.5728571298558395\t Validation Loss: 0.378294532487907\n",
      "\t Training Accuracy: 3800.172916666667 \t Validation Accuracy: 4154.075\n",
      "\n",
      " Epoch: 48\t Training Loss: 0.5773227735050093\t Validation Loss: 0.3847594895788765\n",
      "\t Training Accuracy: 3881.8395833333334 \t Validation Accuracy: 4242.258333333333\n",
      "\n",
      " Epoch: 49\t Training Loss: 0.5742572189764679\t Validation Loss: 0.3518957675230722\n",
      "\t Training Accuracy: 3963.54375 \t Validation Accuracy: 4332.0\n",
      "\n",
      " Epoch: 50\t Training Loss: 0.5588174982710405\t Validation Loss: 0.41307859395490076\n",
      "\t Training Accuracy: 4045.7 \t Validation Accuracy: 4419.25\n",
      "\n",
      " Epoch: 51\t Training Loss: 0.57389547179112\t Validation Loss: 0.3046981847956825\n",
      "\t Training Accuracy: 4127.229166666667 \t Validation Accuracy: 4509.766666666666\n",
      "\n",
      " Epoch: 52\t Training Loss: 0.5600333099034089\t Validation Loss: 0.3162129637162261\n",
      "\t Training Accuracy: 4209.454166666666 \t Validation Accuracy: 4599.366666666667\n",
      "\n",
      " Epoch: 53\t Training Loss: 0.564873728324166\t Validation Loss: 0.35328537459973786\n",
      "\t Training Accuracy: 4291.447916666667 \t Validation Accuracy: 4687.875\n",
      "\n",
      " Epoch: 54\t Training Loss: 0.5722335831408214\t Validation Loss: 0.33870894147316233\n",
      "\t Training Accuracy: 4373.127083333334 \t Validation Accuracy: 4776.866666666667\n",
      "\n",
      " Epoch: 55\t Training Loss: 0.5665358312808366\t Validation Loss: 0.3393881185081579\n",
      "\t Training Accuracy: 4454.927083333333 \t Validation Accuracy: 4866.1\n",
      "\n",
      " Epoch: 56\t Training Loss: 0.5537887711545846\t Validation Loss: 0.34970873040851436\n",
      "\t Training Accuracy: 4537.25625 \t Validation Accuracy: 4954.508333333333\n",
      "\n",
      " Epoch: 57\t Training Loss: 0.5706567272872981\t Validation Loss: 0.3132439765569691\n",
      "\t Training Accuracy: 4619.10625 \t Validation Accuracy: 5044.55\n",
      "\n",
      " Epoch: 58\t Training Loss: 0.5610488097151999\t Validation Loss: 0.36690186937780783\n",
      "\t Training Accuracy: 4701.254166666667 \t Validation Accuracy: 5132.583333333333\n",
      "\n",
      " Epoch: 59\t Training Loss: 0.5650032181846515\t Validation Loss: 0.3167944342299695\n",
      "\t Training Accuracy: 4783.145833333333 \t Validation Accuracy: 5222.425\n",
      "\n",
      " Epoch: 60\t Training Loss: 0.5533162078908569\t Validation Loss: 0.35592206846569063\n",
      "\t Training Accuracy: 4865.30625 \t Validation Accuracy: 5310.933333333333\n",
      "\n",
      " Epoch: 61\t Training Loss: 0.5637168320077397\t Validation Loss: 0.32207392008495817\n",
      "\t Training Accuracy: 4947.20625 \t Validation Accuracy: 5400.541666666667\n",
      "\n",
      " Epoch: 62\t Training Loss: 0.5575103750313806\t Validation Loss: 0.320931351549595\n",
      "\t Training Accuracy: 5029.372916666666 \t Validation Accuracy: 5490.5\n",
      "\n",
      " Epoch: 63\t Training Loss: 0.558481813189847\t Validation Loss: 0.3246232538185979\n",
      "\t Training Accuracy: 5111.575 \t Validation Accuracy: 5579.708333333333\n",
      "\n",
      " Epoch: 64\t Training Loss: 0.5571652901191644\t Validation Loss: 0.34123948308250845\n",
      "\t Training Accuracy: 5193.76875 \t Validation Accuracy: 5669.291666666667\n",
      "\n",
      " Epoch: 65\t Training Loss: 0.5488963804117889\t Validation Loss: 0.3402638548012767\n",
      "\t Training Accuracy: 5276.345833333334 \t Validation Accuracy: 5758.683333333333\n",
      "\n",
      " Epoch: 66\t Training Loss: 0.5551876104888578\t Validation Loss: 0.34515895516250594\n",
      "\t Training Accuracy: 5358.660416666667 \t Validation Accuracy: 5848.033333333334\n",
      "\n",
      " Epoch: 67\t Training Loss: 0.5692218288519694\t Validation Loss: 0.32977941273697875\n",
      "\t Training Accuracy: 5440.291666666667 \t Validation Accuracy: 5937.283333333334\n",
      "\n",
      " Epoch: 68\t Training Loss: 0.5591586954712175\t Validation Loss: 0.30832003766541877\n",
      "\t Training Accuracy: 5522.577083333334 \t Validation Accuracy: 6027.433333333333\n",
      "\n",
      " Epoch: 69\t Training Loss: 0.5675593943138438\t Validation Loss: 0.3043352955954131\n",
      "\t Training Accuracy: 5604.85 \t Validation Accuracy: 6117.966666666666\n",
      "\n",
      " Epoch: 70\t Training Loss: 0.5546973730009767\t Validation Loss: 0.2807903789875533\n",
      "\t Training Accuracy: 5687.108333333334 \t Validation Accuracy: 6209.125\n",
      "\n",
      " Epoch: 71\t Training Loss: 0.5594325340813748\t Validation Loss: 0.3339773743640844\n",
      "\t Training Accuracy: 5769.258333333333 \t Validation Accuracy: 6298.533333333334\n",
      "\n",
      " Epoch: 72\t Training Loss: 0.5610819535194572\t Validation Loss: 0.35612061892343266\n",
      "\t Training Accuracy: 5851.3125 \t Validation Accuracy: 6386.958333333333\n",
      "\n",
      " Epoch: 73\t Training Loss: 0.5516906511192159\t Validation Loss: 0.31357673590217\n",
      "\t Training Accuracy: 5933.789583333333 \t Validation Accuracy: 6476.808333333333\n",
      "\n",
      " Epoch: 74\t Training Loss: 0.5631503521949462\t Validation Loss: 0.3375735126789616\n",
      "\t Training Accuracy: 6015.610416666666 \t Validation Accuracy: 6565.825\n",
      "\n",
      " Epoch: 75\t Training Loss: 0.5612601631640197\t Validation Loss: 0.3362554552631769\n",
      "\t Training Accuracy: 6098.0 \t Validation Accuracy: 6654.741666666667\n",
      "\n",
      " Epoch: 76\t Training Loss: 0.5606234374029793\t Validation Loss: 0.3150345542989694\n",
      "\t Training Accuracy: 6180.41875 \t Validation Accuracy: 6745.141666666666\n",
      "\n",
      " Epoch: 77\t Training Loss: 0.5599396114355679\t Validation Loss: 0.3118956479081052\n",
      "\t Training Accuracy: 6262.375 \t Validation Accuracy: 6835.091666666666\n",
      "\n",
      " Epoch: 78\t Training Loss: 0.5596854184163428\t Validation Loss: 0.33675343679900765\n",
      "\t Training Accuracy: 6344.497916666666 \t Validation Accuracy: 6924.441666666667\n",
      "\n",
      " Epoch: 79\t Training Loss: 0.5486673930735304\t Validation Loss: 0.3396645650571453\n",
      "\t Training Accuracy: 6427.158333333334 \t Validation Accuracy: 7013.6\n",
      "\n",
      " Epoch: 80\t Training Loss: 0.5545048425778948\t Validation Loss: 0.3758116106248345\n",
      "\t Training Accuracy: 6509.479166666667 \t Validation Accuracy: 7101.475\n",
      "\n",
      " Epoch: 81\t Training Loss: 0.5575011965460738\t Validation Loss: 0.32369938005586935\n",
      "\t Training Accuracy: 6591.729166666667 \t Validation Accuracy: 7191.408333333334\n",
      "\n",
      " Epoch: 82\t Training Loss: 0.5626236484837074\t Validation Loss: 0.3468553310811922\n",
      "\t Training Accuracy: 6673.6 \t Validation Accuracy: 7280.35\n",
      "\n",
      " Epoch: 83\t Training Loss: 0.5594132187330435\t Validation Loss: 0.2850559268608377\n",
      "\t Training Accuracy: 6755.90625 \t Validation Accuracy: 7371.608333333334\n",
      "\n",
      " Epoch: 84\t Training Loss: 0.550998816395432\t Validation Loss: 0.2983166284428153\n",
      "\t Training Accuracy: 6838.3625 \t Validation Accuracy: 7461.775\n",
      "\n",
      " Epoch: 85\t Training Loss: 0.5655950765861528\t Validation Loss: 0.3219217696163102\n",
      "\t Training Accuracy: 6920.408333333334 \t Validation Accuracy: 7551.658333333334\n",
      "\n",
      " Epoch: 86\t Training Loss: 0.5435691990484273\t Validation Loss: 0.30265303166239504\n",
      "\t Training Accuracy: 7002.995833333333 \t Validation Accuracy: 7641.8\n",
      "\n",
      " Epoch: 87\t Training Loss: 0.5645411857546778\t Validation Loss: 0.3188727468158684\n",
      "\t Training Accuracy: 7085.077083333334 \t Validation Accuracy: 7731.483333333334\n",
      "\n",
      " Epoch: 88\t Training Loss: 0.5423245586417996\t Validation Loss: 0.29774267001345006\n",
      "\t Training Accuracy: 7167.8875 \t Validation Accuracy: 7822.083333333333\n",
      "\n",
      " Epoch: 89\t Training Loss: 0.5685379615247151\t Validation Loss: 0.35343066243873783\n",
      "\t Training Accuracy: 7249.927083333333 \t Validation Accuracy: 7911.108333333334\n",
      "\n",
      " Epoch: 90\t Training Loss: 0.5501807530219873\t Validation Loss: 0.3303180973040029\n",
      "\t Training Accuracy: 7332.454166666666 \t Validation Accuracy: 8000.133333333333\n",
      "\n",
      " Epoch: 91\t Training Loss: 0.5615156153571594\t Validation Loss: 0.3041232369790548\n",
      "\t Training Accuracy: 7414.83125 \t Validation Accuracy: 8090.658333333334\n",
      "\n",
      " Epoch: 92\t Training Loss: 0.5598970166470559\t Validation Loss: 0.3162128302248341\n",
      "\t Training Accuracy: 7497.014583333334 \t Validation Accuracy: 8180.633333333333\n",
      "\n",
      " Epoch: 93\t Training Loss: 0.5610872659184206\t Validation Loss: 0.3019605158203798\n",
      "\t Training Accuracy: 7579.127083333334 \t Validation Accuracy: 8270.958333333334\n",
      "\n",
      " Epoch: 94\t Training Loss: 0.5427897020586591\t Validation Loss: 0.30319565648562724\n",
      "\t Training Accuracy: 7661.675 \t Validation Accuracy: 8361.566666666668\n",
      "\n",
      " Epoch: 95\t Training Loss: 0.5443081077852271\t Validation Loss: 0.3769136149002651\n",
      "\t Training Accuracy: 7744.2125 \t Validation Accuracy: 8449.916666666666\n",
      "\n",
      " Epoch: 96\t Training Loss: 0.548853251090823\t Validation Loss: 0.42114913345050464\n",
      "\t Training Accuracy: 7826.53125 \t Validation Accuracy: 8536.466666666667\n",
      "\n",
      " Epoch: 97\t Training Loss: 0.5526877269023025\t Validation Loss: 0.31401560723788685\n",
      "\t Training Accuracy: 7908.733333333334 \t Validation Accuracy: 8626.5\n",
      "\n",
      " Epoch: 98\t Training Loss: 0.5456540497739775\t Validation Loss: 0.3036428589579915\n",
      "\t Training Accuracy: 7991.295833333334 \t Validation Accuracy: 8716.925\n",
      "\n",
      " Epoch: 99\t Training Loss: 0.5572016834186447\t Validation Loss: 0.3087166297826334\n",
      "\t Training Accuracy: 8073.4125 \t Validation Accuracy: 8806.625\n",
      "\n",
      " Epoch: 100\t Training Loss: 0.5510025446412211\t Validation Loss: 0.35604514376955804\n",
      "\t Training Accuracy: 8155.84375 \t Validation Accuracy: 8894.875\n",
      "\n",
      " Epoch: 101\t Training Loss: 0.5492731001707706\t Validation Loss: 0.36583477641215534\n",
      "\t Training Accuracy: 8238.439583333333 \t Validation Accuracy: 8983.216666666667\n",
      "\n",
      " Epoch: 102\t Training Loss: 0.5636611357441831\t Validation Loss: 0.3257520712843464\n",
      "\t Training Accuracy: 8320.40625 \t Validation Accuracy: 9072.558333333332\n",
      "\n",
      " Epoch: 103\t Training Loss: 0.5533132314086255\t Validation Loss: 0.31795624349496776\n",
      "\t Training Accuracy: 8402.7625 \t Validation Accuracy: 9162.391666666666\n",
      "\n",
      " Epoch: 104\t Training Loss: 0.5548104309398334\t Validation Loss: 0.3105361807699303\n",
      "\t Training Accuracy: 8485.160416666668 \t Validation Accuracy: 9252.091666666667\n",
      "\n",
      " Epoch: 105\t Training Loss: 0.5555287458799569\t Validation Loss: 0.44426143301331755\n",
      "\t Training Accuracy: 8567.439583333333 \t Validation Accuracy: 9338.616666666667\n",
      "\n",
      " Epoch: 106\t Training Loss: 0.5471141031207779\t Validation Loss: 0.3662425614241171\n",
      "\t Training Accuracy: 8650.05625 \t Validation Accuracy: 9426.691666666668\n",
      "\n",
      " Epoch: 107\t Training Loss: 0.5456471224601592\t Validation Loss: 0.34477595069396094\n",
      "\t Training Accuracy: 8732.504166666668 \t Validation Accuracy: 9515.466666666667\n",
      "\n",
      " Epoch: 108\t Training Loss: 0.5671365738921631\t Validation Loss: 0.29657920503641566\n",
      "\t Training Accuracy: 8814.560416666667 \t Validation Accuracy: 9605.875\n",
      "\n",
      " Epoch: 109\t Training Loss: 0.5603611232124469\t Validation Loss: 0.3063818542145695\n",
      "\t Training Accuracy: 8896.83125 \t Validation Accuracy: 9696.016666666666\n",
      "\n",
      " Epoch: 110\t Training Loss: 0.5544689260486185\t Validation Loss: 0.3461356250508278\n",
      "\t Training Accuracy: 8979.095833333333 \t Validation Accuracy: 9785.241666666667\n",
      "\n",
      " Epoch: 111\t Training Loss: 0.5644670383831033\t Validation Loss: 0.32782402374438324\n",
      "\t Training Accuracy: 9061.2 \t Validation Accuracy: 9875.041666666666\n",
      "\n",
      " Epoch: 112\t Training Loss: 0.54290911021675\t Validation Loss: 0.3101652354676347\n",
      "\t Training Accuracy: 9143.93125 \t Validation Accuracy: 9964.858333333334\n",
      "\n",
      " Epoch: 113\t Training Loss: 0.5589233680423855\t Validation Loss: 0.29411372551964404\n",
      "\t Training Accuracy: 9226.108333333334 \t Validation Accuracy: 10055.408333333333\n",
      "\n",
      " Epoch: 114\t Training Loss: 0.5456708644347341\t Validation Loss: 0.37537385460011996\n",
      "\t Training Accuracy: 9308.70625 \t Validation Accuracy: 10144.033333333333\n",
      "\n",
      " Epoch: 115\t Training Loss: 0.5553915597530251\t Validation Loss: 0.3080121505383347\n",
      "\t Training Accuracy: 9390.96875 \t Validation Accuracy: 10234.566666666668\n",
      "\n",
      " Epoch: 116\t Training Loss: 0.5563153852085357\t Validation Loss: 0.3447518333667175\n",
      "\t Training Accuracy: 9473.416666666666 \t Validation Accuracy: 10323.941666666668\n",
      "\n",
      " Epoch: 117\t Training Loss: 0.5638509822498586\t Validation Loss: 0.32746251987953734\n",
      "\t Training Accuracy: 9555.59375 \t Validation Accuracy: 10413.391666666666\n"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "135d59f5-34bf-4bf7-b498-9a760e615ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, epochs=117):\n",
    "    model.eval()\n",
    "    model.to('cpu')\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to('cpu')\n",
    "            output = model(imgs)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            print(f\"Prediction: {pred}\")\n",
    "            image = imgs.squeeze(0).squeeze(0).numpy()\n",
    "            plt.imshow(image)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50fad911-8045-4dba-a38b-7862766ba155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24f6f1786474023a2cc6b9477cce23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tensor([[7],\n",
      "        [2],\n",
      "        [1],\n",
      "        [0],\n",
      "        [9],\n",
      "        [1],\n",
      "        [9],\n",
      "        [4],\n",
      "        [5],\n",
      "        [9],\n",
      "        [0],\n",
      "        [6],\n",
      "        [9],\n",
      "        [0],\n",
      "        [1],\n",
      "        [5],\n",
      "        [9],\n",
      "        [7],\n",
      "        [3],\n",
      "        [4],\n",
      "        [9],\n",
      "        [6],\n",
      "        [6],\n",
      "        [5],\n",
      "        [9],\n",
      "        [0],\n",
      "        [7],\n",
      "        [9],\n",
      "        [0],\n",
      "        [1],\n",
      "        [3],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [7],\n",
      "        [2],\n",
      "        [7],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1],\n",
      "        [7],\n",
      "        [4],\n",
      "        [2],\n",
      "        [6],\n",
      "        [5],\n",
      "        [1],\n",
      "        [2],\n",
      "        [9],\n",
      "        [4],\n",
      "        [6],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [6],\n",
      "        [0],\n",
      "        [4],\n",
      "        [1],\n",
      "        [9],\n",
      "        [5],\n",
      "        [7],\n",
      "        [2],\n",
      "        [7],\n",
      "        [3],\n",
      "        [7],\n",
      "        [9],\n",
      "        [6],\n",
      "        [4],\n",
      "        [3],\n",
      "        [0],\n",
      "        [7],\n",
      "        [0],\n",
      "        [2],\n",
      "        [9],\n",
      "        [1],\n",
      "        [7],\n",
      "        [3],\n",
      "        [2],\n",
      "        [9],\n",
      "        [7],\n",
      "        [9],\n",
      "        [6],\n",
      "        [2],\n",
      "        [7],\n",
      "        [4],\n",
      "        [4],\n",
      "        [7],\n",
      "        [3],\n",
      "        [6],\n",
      "        [1],\n",
      "        [3],\n",
      "        [6],\n",
      "        [9],\n",
      "        [3],\n",
      "        [1],\n",
      "        [4],\n",
      "        [1],\n",
      "        [7],\n",
      "        [6],\n",
      "        [9],\n",
      "        [6],\n",
      "        [0],\n",
      "        [5],\n",
      "        [4],\n",
      "        [9],\n",
      "        [9],\n",
      "        [2],\n",
      "        [1],\n",
      "        [9],\n",
      "        [4],\n",
      "        [8],\n",
      "        [7],\n",
      "        [3],\n",
      "        [9],\n",
      "        [9],\n",
      "        [9],\n",
      "        [9],\n",
      "        [4],\n",
      "        [9],\n",
      "        [2],\n",
      "        [5],\n",
      "        [9],\n",
      "        [7],\n",
      "        [6],\n",
      "        [7],\n",
      "        [9],\n",
      "        [0],\n",
      "        [5]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (128, 1, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m, in \u001b[0;36mtest_model\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m image \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 11\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mG:\\ml\\venv\\Lib\\site-packages\\matplotlib\\pyplot.py:3592\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[0;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[0;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[0;32m   3572\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3590\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3591\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[1;32m-> 3592\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3596\u001b[0m \u001b[43m        \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3597\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3598\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolorizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3602\u001b[0m \u001b[43m        \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3604\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3608\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3609\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3610\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3612\u001b[0m     sci(__ret)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[1;32mG:\\ml\\venv\\Lib\\site-packages\\matplotlib\\__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[0;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32mG:\\ml\\venv\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:5945\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m-> 5945\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5946\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[0;32m   5947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5948\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[1;32mG:\\ml\\venv\\Lib\\site-packages\\matplotlib\\image.py:675\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    674\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mG:\\ml\\venv\\Lib\\site-packages\\matplotlib\\image.py:643\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    641\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[0;32m    645\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[0;32m    649\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (128, 1, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28eb771-1978-46c6-af11-9babb0f1bb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
